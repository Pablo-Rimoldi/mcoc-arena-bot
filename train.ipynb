{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUhPTUdw-Fbm"
      },
      "source": [
        "# MCOC Action Predictor from Video Frames (Optimized)\n",
        "\n",
        "This notebook trains a deep learning model to predict actions in the game *Marvel Contest of Champions (MCOC)* based on sequences of screen frames. The model uses a pre-trained MobileNetV2 as a CNN backbone to extract features from each frame, followed by a GRU network to understand the temporal sequence of these features.\n",
        "\n",
        "**This version has been optimized for significantly faster GPU training by implementing vectorized operations, Automatic Mixed Precision (AMP), and efficient data loading.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UstIhV2SL2Vu"
      },
      "outputs": [],
      "source": [
        "%pip install mss keyboard opencv-python numpy torch torchvision torchaudio pillow matplotlib scikit-learn seaborn tqdm requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2EE9pMc-Fbp"
      },
      "source": [
        "### 1. Imports\n",
        "\n",
        "First, let's import all the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpg6KUnC-Fbq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from typing import List, Tuple, Dict\n",
        "import json\n",
        "import sys\n",
        "import zipfile\n",
        "import requests\n",
        "from tqdm.notebook import tqdm # <-- OPTIMIZATION: For better progress bars\n",
        "from torch.cuda.amp import GradScaler, autocast # <-- OPTIMIZATION: For Mixed Precision Training\n",
        "\n",
        "RAW_DIR = \"assets/data/raw\"\n",
        "RAW_ZIP_URL = \"https://huggingface.co/datasets/Pablo276/mcoc_labeled_images/resolve/main/raw.zip\"\n",
        "RAW_ZIP_PATH = \"assets/data/raw.zip\"\n",
        "\n",
        "def is_dir_empty(path):\n",
        "    return not os.path.exists(path) or (os.path.isdir(path) and len(os.listdir(path)) == 0)\n",
        "\n",
        "if is_dir_empty(RAW_DIR):\n",
        "    os.makedirs(RAW_DIR, exist_ok=True)\n",
        "    print(f\"Downloading dataset from {RAW_ZIP_URL} ...\")\n",
        "    with requests.get(RAW_ZIP_URL, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        with open(RAW_ZIP_PATH, \"wb\") as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(RAW_ZIP_PATH, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(\"assets/data/\")\n",
        "    os.remove(RAW_ZIP_PATH)\n",
        "    print(\"Dataset ready in assets/data/raw.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLLvd3wf-Fbt"
      },
      "source": [
        "### 2. Configuration\n",
        "\n",
        "Instead of importing from a `config.py` file, we define our configuration parameters directly in the notebook for better portability. This includes paths, model hyperparameters, and data settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqae2K8--Fbu"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    # Directories\n",
        "    DATA_DIR = 'assets/data/raw' # <--- MODIFIED PATH\n",
        "    MODEL_SAVE_DIR = 'models/'\n",
        "    RESULTS_DIR = 'results/'\n",
        "\n",
        "    # Data Parameters\n",
        "    IMAGE_SIZE = 128\n",
        "    SEQUENCE_LENGTH = 10\n",
        "    LABEL_MAPPING = {\n",
        "        'w': 0,  # light_attack\n",
        "        'd': 1,  # medium_attack\n",
        "        'a': 2,  # evade\n",
        "        's': 3,  # parry\n",
        "        'space': 4  # special_attack\n",
        "    }\n",
        "    NUM_CLASSES = len(LABEL_MAPPING)\n",
        "\n",
        "    # Model Hyperparameters\n",
        "    FEATURE_DIM = 256\n",
        "    HIDDEN_SIZE = 128\n",
        "    DROPOUT_RATE = 0.3\n",
        "\n",
        "    # Training Parameters\n",
        "    BATCH_SIZE = 32 # <-- OPTIMIZATION: Increased batch size, possible due to AMP memory savings\n",
        "    LEARNING_RATE = 1e-4\n",
        "    NUM_EPOCHS = 10\n",
        "\n",
        "    # Convert class attributes to a dictionary for saving\n",
        "    @classmethod\n",
        "    def __dict__(cls):\n",
        "        return {k: v for k, v in cls.__dict__.items() if not k.startswith('__') and not callable(v)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAi8I7tT-Fbz"
      },
      "source": [
        "### 4. Dataset Class\n",
        "\n",
        "The `MCOCDataset` class handles loading the data. It takes a list of file paths, creates overlapping sequences of a specified length (`sequence_length`), and extracts the label from the filename of the last frame in each sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai1eHVWx-Fb0"
      },
      "outputs": [],
      "source": [
        "class MCOCDataset(Dataset):\n",
        "    def __init__(self, file_paths: List[str], sequence_length: int = 10, transform=None):\n",
        "        self.file_paths = file_paths\n",
        "        self.sequence_length = sequence_length\n",
        "        self.transform = transform\n",
        "        self.sequences = self._create_sequences()\n",
        "\n",
        "    def _create_sequences(self) -> List[Tuple[List[str], int]]:\n",
        "        \"\"\"Crea sequenze di frame con le relative label\"\"\"\n",
        "        sequences = []\n",
        "\n",
        "        # Raggruppa i file per sequenze consecutive\n",
        "        for i in range(len(self.file_paths) - self.sequence_length + 1):\n",
        "            sequence_files = self.file_paths[i:i + self.sequence_length]\n",
        "\n",
        "            # Estrai la label dal nome del file (ultimo frame della sequenza)\n",
        "            last_file = sequence_files[-1]\n",
        "            label = self._extract_label(last_file)\n",
        "\n",
        "            if label is not None:\n",
        "                sequences.append((sequence_files, label))\n",
        "\n",
        "        return sequences\n",
        "\n",
        "    def _extract_label(self, filename: str) -> int:\n",
        "        \"\"\"Estrae la label dal nome del file\"\"\"\n",
        "        # Estrai la parte dopo l'underscore (e.g., 003730_space.png -> space)\n",
        "        match = re.search(r'_([^.]+)\\.png$', filename)\n",
        "        if match:\n",
        "            label_str = match.group(1)\n",
        "            return Config.LABEL_MAPPING.get(label_str)\n",
        "        return None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence_files, label = self.sequences[idx]\n",
        "\n",
        "        # Carica le immagini della sequenza\n",
        "        images = []\n",
        "        for file_path in sequence_files:\n",
        "            img = Image.open(file_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            images.append(img)\n",
        "\n",
        "        # Stack delle immagini in un tensor (T, C, H, W)\n",
        "        images_tensor = torch.stack(images)\n",
        "\n",
        "        return {\n",
        "            'images': images_tensor,\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG4H5_Qg-Fb1"
      },
      "source": [
        "### 5. Model Architecture (Optimized)\n",
        "\n",
        "The `MCOCActionPredictor` is a hybrid CNN-RNN model.\n",
        "- **CNN Backbone**: A pre-trained `MobileNetV2` is used to extract spatial features from each frame. We freeze the initial layers to leverage pre-trained knowledge and reduce computation.\n",
        "- **Feature Extractor**: A fully connected layer reduces the dimensionality of the features extracted by the CNN.\n",
        "- **RNN (GRU)**: A Gated Recurrent Unit (GRU) network processes the sequence of frame features to capture temporal dependencies.\n",
        "- **Classifier**: A final set of linear layers predicts the action class based on the GRU's output from the last timestep.\n",
        "\n",
        "**Optimization:** The `forward` pass is now **vectorized**. Instead of looping through each frame in the sequence, we reshape the input `(batch, seq_len, C, H, W)` to `(batch * seq_len, C, H, W)` and pass it through the CNN in a single, highly parallel operation. This is significantly faster on a GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kut5B6W-Fb2"
      },
      "outputs": [],
      "source": [
        "class MCOCActionPredictor(nn.Module):\n",
        "    def __init__(self, feature_dim=256, hidden_size=128, num_classes=5, dropout_rate=0.3):\n",
        "        super(MCOCActionPredictor, self).__init__()\n",
        "\n",
        "        # CNN Backbone (MobileNetV2 preaddestrato)\n",
        "        self.cnn = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
        "\n",
        "        # Freeze i primi layer per risparmiare memoria\n",
        "        for param in self.cnn.features[:10].parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Rimuovi il classificatore originale di MobileNetV2\n",
        "        self.cnn.classifier = nn.Identity()\n",
        "\n",
        "        # Feature extractor per ottenere feature_dim dimensioni\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1280, feature_dim),  # 1280 è l'output di MobileNetV2\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "\n",
        "        # RNN (GRU) per la sequenza temporale\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=feature_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=dropout_rate if 2 > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Classificatore finale\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, sequence_length, channels, height, width)\n",
        "        batch_size, seq_len, c, h, w = x.shape\n",
        "\n",
        "        # <-- OPTIMIZATION: Vectorize the CNN pass -->\n",
        "        # Reshape to treat all frames from all sequences as a single batch\n",
        "        x_reshaped = x.view(batch_size * seq_len, c, h, w)\n",
        "\n",
        "        # Pass all frames through the CNN backbone and feature extractor at once\n",
        "        features = self.cnn.features(x_reshaped) # (batch*seq, 1280, H, W)\n",
        "        extracted_features = self.feature_extractor(features) # (batch*seq, feature_dim)\n",
        "\n",
        "        # Reshape back to sequence format for the GRU\n",
        "        # (batch_size, seq_len, feature_dim)\n",
        "        sequence_features = extracted_features.view(batch_size, seq_len, -1)\n",
        "\n",
        "        # Pass the sequence of features through the GRU\n",
        "        # gru_output shape: (batch_size, seq_len, hidden_size)\n",
        "        gru_output, _ = self.gru(sequence_features)\n",
        "\n",
        "        # Use only the output of the last timestep for prediction\n",
        "        final_output = gru_output[:, -1, :]  # (batch_size, hidden_size)\n",
        "\n",
        "        # Final classification\n",
        "        logits = self.classifier(final_output)  # (batch_size, num_classes)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-nCAVmY-Fb3"
      },
      "source": [
        "### 6. Training and Validation Functions (Optimized)\n",
        "\n",
        "These helper functions define the logic for a single training epoch and a single validation epoch.\n",
        "\n",
        "**Optimizations:**\n",
        "- **Automatic Mixed Precision (AMP):** We wrap the forward pass in `with autocast()` to allow PyTorch to use `float16` for faster computations.\n",
        "- **Gradient Scaling:** `GradScaler` is used to prevent underflow issues with `float16` gradients during the backward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hiFpzNH-Fb4"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device, scaler):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # <-- OPTIMIZATION: Use tqdm for a progress bar\n",
        "    progress_bar = tqdm(dataloader, desc='Training', leave=False)\n",
        "    for batch in progress_bar:\n",
        "        images = batch['images'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # <-- OPTIMIZATION: Automatic Mixed Precision (AMP)\n",
        "        with autocast():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        # <-- OPTIMIZATION: Scale loss and backpropagate\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix(loss=total_loss/total, acc=correct/total)\n",
        "\n",
        "    return total_loss / len(dataloader), correct / total\n",
        "\n",
        "\n",
        "def validate_epoch(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc='Validating', leave=False)\n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            images = batch['images'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            # <-- OPTIMIZATION: Use autocast for faster inference as well\n",
        "            with autocast():\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            progress_bar.set_postfix(loss=total_loss/total, acc=correct/total)\n",
        "\n",
        "    return total_loss / len(dataloader), correct / total, all_predictions, all_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc4gfGXk-Fb5"
      },
      "source": [
        "### 7. Main Execution: Data Loading and Preparation\n",
        "\n",
        "Here we set up the device, define the image transformations, load the file paths, and split the data into training, validation, and test sets.\n",
        "\n",
        "**Optimization:** We set `pin_memory=True` in the `DataLoader`. This pre-loads data into pinned (non-pageable) memory, which speeds up the data transfer from the CPU to the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd74r2BG-Fb5"
      },
      "outputs": [],
      "source": [
        "# Setup device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(Config.MODEL_SAVE_DIR, exist_ok=True)\n",
        "os.makedirs(Config.RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "# Data preprocessing and augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[\n",
        "                         0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load and organize data\n",
        "print(\"Loading data...\")\n",
        "all_files = []\n",
        "for filename in os.listdir(Config.DATA_DIR):\n",
        "    if filename.endswith('.png'):\n",
        "        file_path = os.path.join(Config.DATA_DIR, filename)\n",
        "        all_files.append(file_path)\n",
        "\n",
        "# Sort files by number to maintain temporal order\n",
        "all_files = [f for f in all_files if re.search(\n",
        "    r'\\d+', os.path.basename(f))]\n",
        "\n",
        "# Sort files by the first number found\n",
        "all_files.sort(key=lambda x: int(\n",
        "    re.search(r'\\d+', os.path.basename(x)).group(0)))\n",
        "\n",
        "print(f\"Total files found: {len(all_files)}\")\n",
        "\n",
        "# Split data\n",
        "train_files, test_files = train_test_split(\n",
        "    all_files, test_size=0.2, random_state=42, shuffle=False)\n",
        "train_files, val_files = train_test_split(\n",
        "    train_files, test_size=0.2, random_state=42, shuffle=False)\n",
        "\n",
        "print(f\"Train files: {len(train_files)}\")\n",
        "print(f\"Validation files: {len(val_files)}\")\n",
        "print(f\"Test files: {len(test_files)}\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = MCOCDataset(train_files, Config.SEQUENCE_LENGTH, transform)\n",
        "val_dataset = MCOCDataset(val_files, Config.SEQUENCE_LENGTH, transform)\n",
        "test_dataset = MCOCDataset(test_files, Config.SEQUENCE_LENGTH, transform)\n",
        "\n",
        "# <-- OPTIMIZATION: Use more workers and pin_memory=True for faster data loading\n",
        "num_workers = min(os.cpu_count(), 4) # A reasonable number of workers\n",
        "pin_memory = True if device.type == 'cuda' else False\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
        "\n",
        "print(f\"Train sequences: {len(train_dataset)}\")\n",
        "print(f\"Validation sequences: {len(val_dataset)}\")\n",
        "print(f\"Test sequences: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3eJjdri-Fb6"
      },
      "source": [
        "### 8. Model Initialization and Training Loop\n",
        "\n",
        "We initialize the model, loss function (Criterion), optimizer, and a learning rate scheduler. Then, we run the training loop for the specified number of epochs, saving the best-performing model based on validation accuracy.\n",
        "\n",
        "**Optimization:** We initialize `GradScaler` here, which will be passed to the training function to manage mixed-precision training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhlHHWT4-Fb7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.amp\n",
        "import os\n",
        "from tqdm import tqdm # Assicurati che tqdm sia importato\n",
        "\n",
        "# Initialize model\n",
        "model = MCOCActionPredictor(\n",
        "    feature_dim=Config.FEATURE_DIM,\n",
        "    hidden_size=Config.HIDDEN_SIZE,\n",
        "    num_classes=Config.NUM_CLASSES,\n",
        "    dropout_rate=Config.DROPOUT_RATE\n",
        ").to(device)\n",
        "\n",
        "# --- MODIFICA: Calcolo dei pesi per gestire lo sbilanciamento delle classi ---\n",
        "print(\"Calculating class weights for the loss function...\")\n",
        "\n",
        "# 1. Accedi direttamente alla lista di sequenze pre-calcolate nel dataset di training.\n",
        "#    Questo è molto efficiente perché non carichiamo le immagini.\n",
        "class_counts = [0] * Config.NUM_CLASSES\n",
        "for _, label in tqdm(train_dataset.sequences, desc=\"Counting Samples in Train Set\"):\n",
        "    # L'etichetta è il secondo elemento della tupla in train_dataset.sequences\n",
        "    class_counts[label] += 1\n",
        "\n",
        "# 2. Calcola i pesi come l'inverso della frequenza della classe\n",
        "total_samples = sum(class_counts)\n",
        "# Gestiamo il caso in cui una classe non abbia campioni per evitare la divisione per zero\n",
        "class_weights = [total_samples / count if count > 0 else 0 for count in class_counts]\n",
        "weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "print(f\"Class counts found: {class_counts}\")\n",
        "print(f\"Calculated weights for the loss function: {weights_tensor}\")\n",
        "\n",
        "# 3. Usa i pesi calcolati nella loss function\n",
        "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
        "# --------------------------------------------------------------------------\n",
        "\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(), lr=Config.LEARNING_RATE, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "# --- FIX #1: Update GradScaler to the new, recommended API ---\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "# Training loop\n",
        "print(\"Starting training...\")\n",
        "best_val_acc = 0\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "\n",
        "for epoch in range(Config.NUM_EPOCHS):\n",
        "    print(f'Epoch [{epoch+1}/{Config.NUM_EPOCHS}]')\n",
        "    # Training\n",
        "    train_loss, train_acc = train_epoch(\n",
        "        model, train_loader, criterion, optimizer, device, scaler)\n",
        "\n",
        "    # Validation\n",
        "    val_loss, val_acc, _, _ = validate_epoch(\n",
        "        model, val_loader, criterion, device)\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "\n",
        "        # --- FIX #2: Convert Config.__dict__ to a standard, picklable dictionary ---\n",
        "        config_to_save = {k: v for k, v in Config.__dict__.items() if not k.startswith('__')}\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'config': config_to_save\n",
        "        }, os.path.join(Config.MODEL_SAVE_DIR, 'best_model.pth'))\n",
        "\n",
        "    # Log progress\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    print(f'  -> Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "    print(f'  -> Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "    print(f'  -> Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "    print('-' * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybX-NDvy-Fb7"
      },
      "source": [
        "### 9. Plotting Training Curves\n",
        "\n",
        "Visualizing the training and validation loss/accuracy helps in understanding the model's learning progress and diagnosing issues like overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlRPciqP-Fb8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accs, label='Train Accuracy')\n",
        "plt.plot(val_accs, label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(Config.RESULTS_DIR, 'training_curves.png'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gyaapi8-Fb8"
      },
      "source": [
        "### 10. Final Evaluation\n",
        "\n",
        "Finally, we load the best saved model and evaluate its performance on the unseen test set. We print a classification report and display a confusion matrix to analyze its performance on a per-class basis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW2Jv2bP-Fb9"
      },
      "outputs": [],
      "source": [
        "# Test evaluation\n",
        "print(\"Evaluating on test set...\")\n",
        "best_model_path = os.path.join(Config.MODEL_SAVE_DIR, 'best_model.pth')\n",
        "if os.path.exists(best_model_path):\n",
        "    # Load the best model's state dict\n",
        "    checkpoint = torch.load(best_model_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"Loaded best model from epoch {checkpoint['epoch']+1} with validation accuracy: {checkpoint['val_acc']:.4f}\")\n",
        "\n",
        "    # The validation function is used here for evaluation on the test set\n",
        "    test_loss, test_acc, test_predictions, test_labels = validate_epoch(\n",
        "        model, test_loader, criterion, device)\n",
        "\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "    print(f'Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(test_labels, test_predictions,\n",
        "                                target_names=list(Config.LABEL_MAPPING.keys())))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(test_labels, test_predictions)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=list(Config.LABEL_MAPPING.keys()),\n",
        "                yticklabels=list(Config.LABEL_MAPPING.keys()))\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(Config.RESULTS_DIR, 'confusion_matrix.png'))\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"No model found at {best_model_path}. Skipping evaluation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks55JPnj-Fb9"
      },
      "source": [
        "### 11. Save Final Results\n",
        "\n",
        "We save the final metrics and configuration to a JSON file for easy access and reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRz6cf6E-Fb9"
      },
      "outputs": [],
      "source": [
        "results = {\n",
        "    'test_accuracy': test_acc if 'test_acc' in locals() else 'N/A',\n",
        "    'test_loss': test_loss if 'test_loss' in locals() else 'N/A',\n",
        "    'best_val_accuracy': best_val_acc,\n",
        "    'config': Config.__dict__(),\n",
        "    'label_mapping': Config.LABEL_MAPPING\n",
        "}\n",
        "\n",
        "results_path = os.path.join(Config.RESULTS_DIR, 'results.json')\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\\nTraining completed! Results saved in {Config.RESULTS_DIR}\")\n",
        "print(f\"Best model saved in {Config.MODEL_SAVE_DIR}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
